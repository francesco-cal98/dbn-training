{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ssH1fXDXgRB"
   },
   "source": [
    "# Cognition & Computation - Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udd27 Setup for Google Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running on Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\ud83d\udd27 Running on Google Colab - setting up environment...\\n\")\n",
    "    \n",
    "    # Clone repository if not already done\n",
    "    if not os.path.exists('groundeep-unimodal-training'):\n",
    "        print(\"\ud83d\udce5 Cloning repository...\")\n",
    "        !git clone https://github.com/francesco-cal98/dbn-training.git groundeep-unimodal-training\n",
    "    \n",
    "    # Change to repo directory\n",
    "    os.chdir('groundeep-unimodal-training')\n",
    "    print(f\"\ud83d\udcc2 Working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Install minimal dependencies\n",
    "    print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
    "    !pip install -q torch torchvision numpy matplotlib scikit-learn tqdm\n",
    "    \n",
    "    print(\"\\n\u2705 Setup complete! Ready to run the notebook.\\n\")\n",
    "else:\n",
    "    print(\"\ud83d\udcbb Running locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As usual, let's first download the scripts from the GitHub repository that implement a DBN in PyTorch, and load some useful Python libraries."
   ],
   "metadata": {
    "id": "m-44kecibykE"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sEx-vmM5XCRx"
   },
   "source": [
    "def get_dbn_library():",
    "  \"\"\"",
    "  Load DBN library from local implementation.",
    "  ",
    "  This uses the wrapper classes (DBN.py, RBM.py) which internally use",
    "  the implementation from src/classes/gdbn_model.py.",
    "  ",
    "  No download needed - files are already in the repository!",
    "  \"\"\"",
    "  import os",
    "  ",
    "  # Check if wrapper files exist",
    "  if not os.path.exists('DBN.py'):",
    "    raise FileNotFoundError(",
    "      \"DBN.py not found. Make sure you're running from the repository root.\"",
    "    )",
    "  if not os.path.exists('RBM.py'):",
    "    raise FileNotFoundError(",
    "      \"RBM.py not found. Make sure you're running from the repository root.\"",
    "    )",
    "  ",
    "  print(\"\u2705 Using local DBN implementation\")",
    "  print(\"   - Wrapper classes: DBN.py, RBM.py\")",
    "  print(\"   - Core implementation: src/classes/gdbn_model.py\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "get_dbn_library()"
   ],
   "metadata": {
    "id": "-a7RICztJljG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K9zAAXswX6W_"
   },
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "from DBN import DBN"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's choose the kind of device used for computations (CPU or GPU)."
   ],
   "metadata": {
    "id": "GqtVYVUkcHZ8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "id": "oNV2cc6il-zv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's download and normalize the MNIST dataset as in the previous Lab."
   ],
   "metadata": {
    "id": "BrbDLPIhcNyf"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sQ9eMI7bYEUZ"
   },
   "source": [
    "%%capture\n",
    "mnist_tr = tv.datasets.MNIST(root=\"../mnist\", train=True, download=True)\n",
    "mnist_te = tv.datasets.MNIST(root=\"../mnist\", train=False, download=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mnist_tr.data = mnist_tr.data / 255\n",
    "mnist_te.data = mnist_te.data / 255\n",
    "\n",
    "mnist_tr.data = mnist_tr.data.to(device)\n",
    "mnist_te.data = mnist_te.data.to(device)\n",
    "mnist_tr.targets = mnist_tr.targets.to(device)\n",
    "mnist_te.targets = mnist_te.targets.to(device)"
   ],
   "metadata": {
    "id": "GWWayyi6caaU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now create a hierarchical generative model (Deep Belief Network) and train it in an unsupervised way on the MNIST dataset."
   ],
   "metadata": {
    "id": "fRAzcxiZcnGK"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SBWXVpfkYI2Q"
   },
   "source": [
    "dbn_mnist = DBN(visible_units=28*28,\n",
    "                hidden_units=[400, 500, 800],\n",
    "                k=1,\n",
    "                learning_rate=0.1,\n",
    "                learning_rate_decay=False,\n",
    "                initial_momentum=0.5,\n",
    "                final_momentum=0.95,\n",
    "                weight_decay=0.0001,\n",
    "                xavier_init=False,\n",
    "                increase_to_cd_k=False,\n",
    "                use_gpu=torch.cuda.is_available())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0BfmZ39NYLTV"
   },
   "source": [
    "num_epochs = 50\n",
    "batch_size = 125\n",
    "\n",
    "dbn_mnist.train_static(\n",
    "    mnist_tr.data,\n",
    "    mnist_tr.targets,\n",
    "    num_epochs,\n",
    "    batch_size\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zRN90izXq9A"
   },
   "source": [
    "## Linear read-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI68htznZWBW"
   },
   "source": [
    "As in the previous lab, we can now extract the hidden representations of the data, by propagating the neuron's activations from the sensory (visible) layer in a bottom-up fashion, and try to lineary decode the content of the representations in a supervised way."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_kth_layer_repr(input, k, device):\n",
    "  flattened_input = input.view((input.shape[0], -1)).type(torch.FloatTensor).to(device)\n",
    "  hidden_repr, __ = dbn_mnist.rbm_layers[k].to_hidden(flattened_input)  # here we access the RBM object\n",
    "  return hidden_repr"
   ],
   "metadata": {
    "id": "Vo9_GHi5eqJG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "hidden_repr_1 = get_kth_layer_repr(mnist_tr.data, 0, device)\n",
    "hidden_repr_2 = get_kth_layer_repr(hidden_repr_1, 1, device)\n",
    "hidden_repr_3 = get_kth_layer_repr(hidden_repr_2, 2, device)"
   ],
   "metadata": {
    "id": "ZQ6YRrdces7-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aY50h0yKYbsy"
   },
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "  def __init__(self, last_layer_size):\n",
    "    super().__init__()\n",
    "    self.linear = torch.nn.Linear(last_layer_size, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "layer_size = dbn_mnist.rbm_layers[0].W.shape[1]\n",
    "linear1 = LinearModel(layer_size).to(device)\n",
    "\n",
    "layer_size = dbn_mnist.rbm_layers[1].W.shape[1]\n",
    "linear2 = LinearModel(layer_size).to(device)\n",
    "\n",
    "layer_size = dbn_mnist.rbm_layers[2].W.shape[1]\n",
    "linear3 = LinearModel(layer_size).to(device)"
   ],
   "metadata": {
    "id": "b40x_TokeUbu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_supervised(network, input_data, epochs=1000):\n",
    "  optimizer = torch.optim.SGD(network.parameters(), lr=0.05)\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = network(input_data).squeeze()\n",
    "    targets = mnist_tr.targets.reshape(predictions.shape[0])  # here are the labels\n",
    "    loss = loss_fn(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"epoch : {:3d}/{}, loss = {:.4f}\".format(epoch + 1, epochs, loss))"
   ],
   "metadata": {
    "id": "aIaZ0BrDeZDS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_supervised(linear1, hidden_repr_1, 1000)\n",
    "train_supervised(linear2, hidden_repr_2, 1000)\n",
    "train_supervised(linear3, hidden_repr_3, 1000)"
   ],
   "metadata": {
    "id": "gcKADqOwehz0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "hidden_repr_1_test = get_kth_layer_repr(mnist_te.data, 0, device)\n",
    "hidden_repr_2_test = get_kth_layer_repr(hidden_repr_1_test, 1, device)\n",
    "hidden_repr_3_test = get_kth_layer_repr(hidden_repr_2_test, 2, device)"
   ],
   "metadata": {
    "id": "98jypbsCe1xU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# compute the classifier predictions:\n",
    "predictions_test1 = linear1(hidden_repr_1_test)\n",
    "predictions_test2 = linear2(hidden_repr_2_test)\n",
    "predictions_test3 = linear3(hidden_repr_3_test)"
   ],
   "metadata": {
    "id": "j6dE-BqPe3qz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_accuracy(predictions_test, targets):\n",
    "  predictions_indices = predictions_test.max(axis=1).indices  # convert probabilities to indices\n",
    "  accuracy = (predictions_indices == targets).sum() / len(targets)\n",
    "  return accuracy.item()"
   ],
   "metadata": {
    "id": "TJoVirhgicoa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "compute_accuracy(predictions_test1, mnist_te.targets)"
   ],
   "metadata": {
    "id": "DWUb5-SEop4Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "compute_accuracy(predictions_test2, mnist_te.targets)"
   ],
   "metadata": {
    "id": "pJ5W7AeKox-i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "compute_accuracy(predictions_test3, mnist_te.targets)"
   ],
   "metadata": {
    "id": "yQkNuYnyoz04"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "690R4NNKfNhW"
   },
   "source": [
    "## Comparison with a feed-forward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CYrA1XsgYxG"
   },
   "source": [
    "Let's now train a simple feed-forward neural network with the same structure of the DBN, in order to compare a non-linear model that is trained end-to-end to solve a classification task with a simple linear classifier that solves the same task using representations of input data learned in an unsupervised way through the DBN."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NlR9LznufMp_"
   },
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "  def __init__(self, first_hidden_layer_size, second_hidden_layer_size, third_hidden_layer_size):\n",
    "    super().__init__()\n",
    "    self.first_hidden = torch.nn.Linear(784, first_hidden_layer_size)\n",
    "    self.second_hidden = torch.nn.Linear(first_hidden_layer_size, second_hidden_layer_size)\n",
    "    self.third_hidden = torch.nn.Linear(second_hidden_layer_size, third_hidden_layer_size)\n",
    "    self.output = torch.nn.Linear(third_hidden_layer_size, 10)\n",
    "\n",
    "  def forward(self, input):\n",
    "    relu = torch.nn.ReLU()\n",
    "    first_hidden_repr = relu(self.first_hidden(input))\n",
    "    second_hidden_repr = relu(self.second_hidden(first_hidden_repr))\n",
    "    third_hidden_repr = relu(self.third_hidden(second_hidden_repr))\n",
    "    output = self.output(third_hidden_repr)\n",
    "    return output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Upl8aiD-gqP3"
   },
   "source": [
    "ffnn = Feedforward(400, 500, 800).to(device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ccEZ2ZMFiBrX"
   },
   "source": [
    "train_supervised(ffnn, mnist_tr.data.reshape((60000, 784)), 1050)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_lDbHNfqiTgo"
   },
   "source": [
    "predictions_ffnn = ffnn(mnist_te.data.reshape((10000, 784)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CExo7uyiiVgr"
   },
   "source": [
    "compute_accuracy(predictions_ffnn, mnist_te.targets)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73eq5eNPv4xZ"
   },
   "source": [
    "## Robustness to noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oitTC0d4v6sx"
   },
   "source": [
    "We will now inject some noise in the input images and see how much the representations learned by the DBN and the feed-forward network are robust to perturbations in the sensory signal.\n",
    "\n",
    "Similarly to what happens in psychophysical experiments, this will allow to create a psychometric curve describing the decrease in classification accuracy with respect to the noise level."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1EYKuRtVwLIs"
   },
   "source": [
    "def inject_noise(mnist_data, noise_level):\n",
    "\n",
    "  ### TASK: create a very simple function that adds some Gaussian noise (see torch.randn function) to the MNIST data\n",
    "  random_gaussian_tensor = torch.randn(mnist_data.shape, device = device)*noise_level\n",
    "  return mnist_data + random_gaussian_tensor"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0tSj95s0HfR"
   },
   "source": [
    "Let's see what a noisy image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YnnaCxzRzFX1"
   },
   "source": [
    "noise_level = 0.3\n",
    "mnist_test_with_noise = inject_noise(mnist_te.data, noise_level)\n",
    "__ = plt.imshow(mnist_test_with_noise[0].reshape(28, 28).to(\"cpu\"), cmap=\"gray\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO9V8x1CiaR4"
   },
   "source": [
    "We will now compute the hidden representations for the noisy images using the DBN. Then, we will use the read-out classifiers that we trained on the representations without noise to classify the noisy stimuli."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_accuracy_values_at_noise_level(noise_level):\n",
    "\n",
    "  mnist_test_with_noise = inject_noise(mnist_te.data, noise_level)  # first, let's create noisy test images\n",
    "\n",
    "  hidden_repr_1_noisy = get_kth_layer_repr(mnist_test_with_noise, 0, device)  # here we compute the DBN representations\n",
    "  hidden_repr_2_noisy = get_kth_layer_repr(hidden_repr_1_noisy, 1, device)\n",
    "  hidden_repr_3_noisy = get_kth_layer_repr(hidden_repr_2_noisy, 2, device)\n",
    "\n",
    "  predictions_first_hidden_noisy = linear1(hidden_repr_1_noisy)  # here we use the previously-trained read-out classifiers\n",
    "  predictions_second_hidden_noisy = linear2(hidden_repr_2_noisy)\n",
    "  predictions_third_hidden_noisy = linear3(hidden_repr_3_noisy)\n",
    "\n",
    "  accuracy_first_hidden = compute_accuracy(predictions_first_hidden_noisy, mnist_te.targets)\n",
    "  accuracy_second_hidden = compute_accuracy(predictions_second_hidden_noisy, mnist_te.targets)\n",
    "  accuracy_third_hidden = compute_accuracy(predictions_third_hidden_noisy, mnist_te.targets)\n",
    "\n",
    "  ### TASK: repeat a similar process for the feed-forward model (NB: make sure you reshape the input data appropriately!)\n",
    "  predictions_ffnn_noisy = ffnn(mnist_test_with_noise.reshape((10000, 784)))  # we repeat a similar process for the feed-forward model\n",
    "  accuracy_ffnn = compute_accuracy(predictions_ffnn_noisy, mnist_te.targets)\n",
    "\n",
    "  return accuracy_first_hidden, accuracy_second_hidden, accuracy_third_hidden, accuracy_ffnn"
   ],
   "metadata": {
    "id": "Od4e7f8YQ3wW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oTiusj9Dzp8X"
   },
   "source": [
    "acc = get_accuracy_values_at_noise_level(0.3);\n",
    "print(\"Accuracy of H1 read-out: %.3f\" % acc[0])\n",
    "print(\"Accuracy of H2 read-out: %.3f\" % acc[1])\n",
    "print(\"Accuracy of H3 read-out: %.3f\" % acc[2])\n",
    "print(\"Accuracy of FF network : %.3f\" % acc[3])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKZPC2Wx0rH_"
   },
   "source": [
    "Let's create the psychometric curves for the DBN (at different levels of internal representations) and for the feed-forward network:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eUwuzAQYqIyB"
   },
   "source": [
    "def plot_noise_robustness_curves(noise_levels):\n",
    "  accuracy_values_first_hidden = []\n",
    "  accuracy_values_second_hidden = []\n",
    "  accuracy_values_third_hidden = []\n",
    "  accuracy_values_ffnn = []\n",
    "\n",
    "  for noise_level in noise_levels:\n",
    "    acc = get_accuracy_values_at_noise_level(noise_level)\n",
    "    accuracy_values_first_hidden.append(acc[0])\n",
    "    accuracy_values_second_hidden.append(acc[1])\n",
    "    accuracy_values_third_hidden.append(acc[2])\n",
    "    accuracy_values_ffnn.append(acc[3])\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(range(len(noise_levels)), accuracy_values_first_hidden)\n",
    "  ax.plot(range(len(noise_levels)), accuracy_values_second_hidden)\n",
    "  ax.plot(range(len(noise_levels)), accuracy_values_third_hidden)\n",
    "  ax.plot(range(len(noise_levels)), accuracy_values_ffnn)\n",
    "\n",
    "  ax.set_title(\"Robustness to noise\")\n",
    "  ax.set_xlabel(\"Noise level (%)\")\n",
    "  ax.set_ylabel(\"Accuracy\")\n",
    "  plt.xticks(range(len(noise_levels)), [int(l*100) for l in noise_levels])\n",
    "  plt.legend([\"First hidden\", \"Second hidden\", \"Third hidden\", \"FFNN\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U63ZIXbKy2DC"
   },
   "source": [
    "noise_levels = np.linspace(0,2,10)\n",
    "plot_noise_robustness_curves(noise_levels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reference paper\n",
    "- [Testolin et al. (2017) - Letter perception emerges from unsupervised deep learning and recycling of natural image features](https://www.nature.com/articles/s41562-017-0186-2)"
   ],
   "metadata": {
    "id": "87pG15YgKjtd"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC71fHwV6zEr"
   },
   "source": [
    "## Contacts\n",
    "\n",
    "- \ud83d\udce7 flavio.petruzzellis@phd.unipd.it\n",
    "- \ud83d\udcac [Moodle forum](https://stem.elearning.unipd.it/mod/forum/view.php?id=600538)"
   ]
  }
 ]
}